{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network for classification of contaminants with MAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Formulate/outline the problem: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple neural network for classification of the contaminants using MAT transcriptomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"gene_counts_NN_training.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sample\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data.drop(columns=[\"sample\"])\n",
    "target = data[\"sample\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_features, target, test_size=0.2, random_state=0, shuffle=True, stratify=target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(), y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_normalize_DESeq2_style(data):\n",
    "    # Ensure all values are non-negative\n",
    "    data = data.applymap(lambda x: max(x, 0))\n",
    "\n",
    "    # Take the log\n",
    "    log_data = np.log1p(data)\n",
    "\n",
    "    # Calculate the pseudo-reference sample for each gene\n",
    "    log_data[\"pseudo_reference\"] = log_data.mean(axis=1)\n",
    "\n",
    "    # Filter out genes with -Inf as their average\n",
    "    filtered_log_data = log_data[log_data[\"pseudo_reference\"] != float(\"-inf\")]\n",
    "\n",
    "    # Subtract the gene pseudo-references from log counts\n",
    "    ratio_data = filtered_log_data.iloc[:, :-1].sub(\n",
    "        filtered_log_data[\"pseudo_reference\"], axis=0\n",
    "    )\n",
    "\n",
    "    # Find the median of the ratios for each sample\n",
    "    sample_medians = ratio_data.median(axis=0)\n",
    "\n",
    "    # Convert medians to scaling factors\n",
    "    scaling_factors = np.exp(sample_medians)\n",
    "\n",
    "    # Divide the original counts by the scaling factors\n",
    "    manually_normalized = data.div(scaling_factors)\n",
    "\n",
    "    return manually_normalized\n",
    "\n",
    "\n",
    "X_train = to_normalize_DESeq2_style(X_train)\n",
    "\n",
    "X_test = to_normalize_DESeq2_style(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "encoded_labels = le.transform(y_train)\n",
    "le_name_mapping = dict(zip(le.classes_, encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Define feature selection\n",
    "fs = SelectKBest(score_func=chi2, k=500)\n",
    "\n",
    "# Apply feature selection\n",
    "X_train = fs.fit_transform(X_train, encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=encoded_labels, cmap=\"Set1\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA Visualization of Selected Features\")\n",
    "plt.colorbar(label=\"Class Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test feature matrix\n",
    "X_test = fs.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y_train, dtype=int)\n",
    "y_test = pd.get_dummies(y_test, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build an architecture from scratch or choose a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "keras.utils.set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_NOTEBOOK_NAME = \"20230829 NN with Wandb.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    \"hidden_units\": 56,\n",
    "    \"dropout_rates\": 0.2,\n",
    "    \"batch_size\": 15,\n",
    "    \"activation\": \"relu\",\n",
    "    \"weights_limit\": 0.01,\n",
    "    \"epochs\": 300,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss\": \"categorical_crossentropy\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"NN-MATseq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "def create_nn():\n",
    "    with wandb.init(config=default_config) as run:\n",
    "        dropout_rates = run.config.dropout_rates\n",
    "        batch_size = run.config.batch_size\n",
    "        activation = run.config.activation\n",
    "        weights_limit = run.config.weights_limit\n",
    "        epochs = run.config.epochs\n",
    "        learning_rate = run.config.learning_rate\n",
    "        loss = run.config.loss\n",
    "        hidden_units = run.config.hidden_units\n",
    "\n",
    "        inputs = keras.Input(shape=X_train.shape[1])\n",
    "        x = keras.layers.UnitNormalization()(inputs)\n",
    "\n",
    "        x = keras.layers.Dense(\n",
    "            config.get(\"hidden_units\"),\n",
    "            activation=config.get(\"activation\"),\n",
    "            kernel_regularizer=keras.regularizers.L2(config.get(\"weights_limit\")),\n",
    "        )(x)\n",
    "        x = keras.layers.Dropout(config.get(\"dropout_rates\"))(x)\n",
    "\n",
    "        x = keras.layers.Dense(\n",
    "            config.get(\"hidden_units\"),\n",
    "            activation=config.get(\"activation\"),\n",
    "            kernel_regularizer=keras.regularizers.L2(config.get(\"weights_limit\")),\n",
    "        )(x)\n",
    "        x = keras.layers.Dropout(config.get(\"dropout_rates\"))(x)\n",
    "\n",
    "        outputs = keras.layers.Dense(9, activation=\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=\"small_NN\")\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=config.get(\"learning_rate\"))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=config.get(\"loss\"),\n",
    "            metrics=[\n",
    "                \"accuracy\",\n",
    "                keras.metrics.AUC(name=\"auc\"),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=config.get(\"epochs\"),\n",
    "            callbacks=[WandbCallback(), early_stop],\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "\n",
    "model = create_nn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MATseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
