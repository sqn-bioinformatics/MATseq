{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network for classification of contaminants with MAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Formulate/outline the problem: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple neural network for classification of the contaminants using MAT transcriptomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"gene_counts_NN_training.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data.drop(columns=[\"sample\"])\n",
    "target = data[\"sample\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_features, target, test_size=0.2, random_state=0, shuffle=True, stratify=target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_normalize_DESeq2_style(data):\n",
    "    # Ensure all values are non-negative\n",
    "    data = data.applymap(lambda x: max(x, 0))\n",
    "\n",
    "    # Take the log\n",
    "    log_data = np.log1p(data)\n",
    "\n",
    "    # Calculate the pseudo-reference sample for each gene\n",
    "    log_data[\"pseudo_reference\"] = log_data.mean(axis=1)\n",
    "\n",
    "    # Filter out genes with -Inf as their average\n",
    "    filtered_log_data = log_data[log_data[\"pseudo_reference\"] != float(\"-inf\")]\n",
    "\n",
    "    # Subtract the gene pseudo-references from log counts\n",
    "    ratio_data = filtered_log_data.iloc[:, :-1].sub(\n",
    "        filtered_log_data[\"pseudo_reference\"], axis=0\n",
    "    )\n",
    "\n",
    "    # Find the median of the ratios for each sample\n",
    "    sample_medians = ratio_data.median(axis=0)\n",
    "\n",
    "    # Convert medians to scaling factors\n",
    "    scaling_factors = np.exp(sample_medians)\n",
    "\n",
    "    # Divide the original counts by the scaling factors\n",
    "    manually_normalized = data.div(scaling_factors)\n",
    "\n",
    "    return manually_normalized\n",
    "\n",
    "\n",
    "X_train = to_normalize_DESeq2_style(X_train)\n",
    "\n",
    "X_test = to_normalize_DESeq2_style(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the class names and transform them into integers\n",
    "encoded_labels = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Define feature selection\n",
    "fs = SelectKBest(score_func=chi2, k=300)\n",
    "\n",
    "# Apply feature selection\n",
    "X_train = fs.fit_transform(X_train, encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=encoded_labels, cmap=\"viridis\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA Visualization of Selected Features\")\n",
    "plt.colorbar(label=\"Class Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test feature matrix\n",
    "X_test = fs.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y_train, dtype=int)\n",
    "y_test = pd.get_dummies(y_test, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build an architecture from scratch or choose a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "keras.utils.set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create our model\n",
    "# def create_nn(learning_rate=0.001):\n",
    "#     inputs = keras.Input(shape=X_train.shape[1])\n",
    "#     x = keras.layers.UnitNormalization()(inputs)\n",
    "#     x = keras.layers.Dropout(0.3)(x)\n",
    "#     x = keras.layers.Dense(\n",
    "#         256, activation=\"relu\", kernel_regularizer=keras.regularizers.L2(0.01)\n",
    "#     )(x)\n",
    "#     x = keras.layers.BatchNormalization()(x)\n",
    "#     x = keras.layers.Dropout(0.2)(x)\n",
    "#     x = keras.layers.Dense(\n",
    "#         128, activation=\"relu\", kernel_regularizer=keras.regularizers.L2(0.01)\n",
    "#     )(x)\n",
    "#     x = keras.layers.Dropout(0.1)(x)\n",
    "#     outputs = keras.layers.Dense(9, activation=\"softmax\")(x)\n",
    "\n",
    "#     model = keras.Model(inputs=inputs, outputs=outputs, name=\"small_NN\")\n",
    "\n",
    "#     optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss=keras.losses.CategoricalCrossentropy(),\n",
    "#         metrics=[\"accuracy\", keras.metrics.AUC()],\n",
    "#     )\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# model = create_nn()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, UnitNormalization, Activation\n",
    "\n",
    "\n",
    "def create_nn(\n",
    "    hidden_units, dropout_rates, num_layers, activation, optimizer, weights_limit\n",
    "):\n",
    "    model = Sequential(name=\"small_NN\")\n",
    "    model.add(UnitNormalization())\n",
    "    model.add(\n",
    "        Dense(hidden_units, input_dim=300),\n",
    "    )\n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(\n",
    "            Dense(hidden_units, kernel_regularizer=keras.regularizers.L2(weights_limit))\n",
    "        )\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dropout(dropout_rates))\n",
    "\n",
    "    model.add(Dense(9, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", keras.metrics.AUC(), keras.metrics.Recall()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_schedule(epoch, lr):\n",
    "    if epoch < 50:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.9\n",
    "\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(learning_rate_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choose a loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"hidden_units\": [8, 16, 24, 56],\n",
    "    \"dropout_rates\": [0.2],\n",
    "    \"num_layers\": [2, 3, 4],\n",
    "    \"batch_size\": [5, 10, 15],\n",
    "    \"optimizer\": [\"adam\"],\n",
    "    \"activation\": [\"relu\", \"tanh\", \"swish\", \"gelu\"],\n",
    "    \"weights_limit\": [0.01, 0.001],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KerasClassifier for grid search\n",
    "keras_clf = keras.wrappers.scikit_learn.KerasClassifier(\n",
    "    build_fn=create_nn, epochs=300, verbose=1\n",
    ")\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = sklearn.model_selection.GridSearchCV(\n",
    "    estimator=keras_clf, param_grid=param_grid, cv=3, scoring=\"f1_micro\", verbose=1\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_result = grid_search.fit(\n",
    "    X_train, encoded_labels, callbacks=[lr_scheduler, early_stop]\n",
    ")\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters: \", grid_result.best_params_)\n",
    "print(\"Best Score: \", grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "def create_nn(hidden_units, dropout_rates, num_layers, activation, optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(hidden_units, input_dim=300))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout_rates))\n",
    "\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(hidden_units))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dropout(dropout_rates))\n",
    "\n",
    "    model.add(Dense(9, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def learning_rate_schedule(epoch, lr):\n",
    "    if epoch < 50:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.9\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "\n",
    "# Create a KerasClassifier for grid search\n",
    "keras_clf = keras.wrappers.scikit_learn.KerasClassifier(\n",
    "    build_fn=create_nn, epochs=300, verbose=0\n",
    ")\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = sklearn.model_selection.GridSearchCV(\n",
    "    estimator=keras_clf, param_grid=param_grid, cv=3, scoring=\"accuracy\", verbose=0\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_result = grid_search.fit(X_train, y_train, callbacks=[lr_scheduler])\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters: \", grid_result.best_params_)\n",
    "print(\"Best Score: \", grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, epochs=300, callbacks=[callback], validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, metrics):\n",
    "    history_df = pd.DataFrame.from_dict(history.history)\n",
    "    sns.lineplot(data=history_df[metrics])\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame.from_dict(history.history)\n",
    "history_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, [\"accuracy\", \"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, [\"loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, [\"auc\", \"val_auc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC (Area under the curve) of the ROC (Receiver operating characteristic; default) or PR (Precision Recall) curves are quality measures of binary classifiers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MATseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
